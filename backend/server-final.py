from flask import Flask, request, jsonify
from flask_cors import CORS
from PIL import Image, ImageFilter, ImageStat
import torch
from transformers import BlipProcessor, BlipForConditionalGeneration
from transformers import ViTImageProcessor, ViTForImageClassification
from transformers import pipeline  # Utiliser pipeline pour la traduction
import logging
from datetime import datetime
import io
import json
import threading
import time

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

# Variables globales pour les modÃ¨les
blip_processor = None
blip_model = None
vit_processor = None
vit_model = None
translation_pipelines = {}
translation_models_loading = False

def load_models_async():
    """Charger les modÃ¨les en arriÃ¨re-plan"""
    global blip_processor, blip_model, vit_processor, vit_model, translation_pipelines, translation_models_loading
    
    logger.info("ğŸ”„ DÃ©but du chargement asynchrone des modÃ¨les...")
    
    try:
        # Charger les modÃ¨les d'analyse d'image
        logger.info("Chargement BLIP...")
        blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
        blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large")
        
        logger.info("Chargement ViT...")
        vit_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')
        vit_model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')
        
        logger.info("âœ… ModÃ¨les d'analyse d'image chargÃ©s")
        
        # Charger les pipelines de traduction (plus simple que MarianMT)
        logger.info("Chargement des pipelines de traduction IA...")
        
        try:
            # Pipeline Anglais -> FranÃ§ais
            translation_pipelines['en-fr'] = pipeline(
                "translation_en_to_fr", 
                model="Helsinki-NLP/opus-mt-en-fr",
                device=-1  # CPU, utiliser device=0 pour GPU
            )
            logger.info("âœ… Pipeline EN->FR chargÃ©")
        except Exception as e:
            logger.warning(f"âš ï¸ Erreur pipeline EN->FR: {e}")
            # Fallback Ã  un modÃ¨le plus lÃ©ger
            try:
                translation_pipelines['en-fr'] = pipeline(
                    "translation", 
                    model="t5-small",
                    tokenizer="t5-small",
                    framework="pt"
                )
                logger.info("âœ… Pipeline T5-small (EN->FR) chargÃ© comme fallback")
            except Exception as e2:
                logger.error(f"âŒ Erreur fallback EN->FR: {e2}")
        
        try:
            # Pipeline Anglais -> Arabe
            translation_pipelines['en-ar'] = pipeline(
                "translation_en_to_ar",
                model="Helsinki-NLP/opus-mt-en-ar",
                device=-1
            )
            logger.info("âœ… Pipeline EN->AR chargÃ©")
        except Exception as e:
            logger.warning(f"âš ï¸ Erreur pipeline EN->AR: {e}")
        
        logger.info("ğŸ‰ Tous les modÃ¨les IA chargÃ©s avec succÃ¨s")
        
    except Exception as e:
        logger.error(f"âŒ Erreur chargement modÃ¨les: {e}")
    
    translation_models_loading = False

# DÃ©marrer le chargement asynchrone des modÃ¨les
threading.Thread(target=load_models_async, daemon=True).start()

# Dictionnaires de traduction
TRANSLATIONS = {
    'fr': {
        'technical_analysis': "Analyse Technique",
        'executive_summary': "RÃ©sumÃ© ExÃ©cutif",
        'content_analysis': "Analyse du Contenu",
        'recommendations': "Recommandations",
        'main_categories': "CatÃ©gories Principales",
        'visual_analysis': "Analyse Visuelle",
        'resolution': "RÃ©solution",
        'aspect_ratio': "Ratio d'Aspect",
        'dominant_colors': "Couleurs Dominantes",
        'composition_type': "Type de Composition",
        'brightness_level': "Niveau de LuminositÃ©",
        'high_contrast': "Contraste Ã©levÃ© amÃ©liore la clartÃ©",
        'sharp_details': "DÃ©tails nets et bords dÃ©finis",
        'color_harmony': "Harmonie colorimÃ©trique multi-spectrale",
        'architectural_elements': "Ã‰lÃ©ments architecturaux structurÃ©s",
        'suitable_for': "AdaptÃ© pour",
        'professional_use': "Usage professionnel",
        'high_quality': "Haute qualitÃ©"
    },
    'ar': {
        'technical_analysis': "Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙ†ÙŠ",
        'executive_summary': "Ù…Ù„Ø®Øµ ØªÙ†ÙÙŠØ°ÙŠ", 
        'content_analysis': "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰",
        'recommendations': "Ø§Ù„ØªÙˆØµÙŠØ§Øª",
        'main_categories': "Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©",
        'visual_analysis': "Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ØµØ±ÙŠ",
        'resolution': "Ø§Ù„Ø¯Ù‚Ø©",
        'aspect_ratio': "Ù†Ø³Ø¨Ø© Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯",
        'dominant_colors': "Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø§Ù„Ù…Ø³ÙŠØ·Ø±Ø©",
        'composition_type': "Ù†ÙˆØ¹ Ø§Ù„ØªØ±ÙƒÙŠØ¨",
        'brightness_level': "Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø³Ø·ÙˆØ¹",
        'high_contrast': "ØªØ¨Ø§ÙŠÙ† Ø¹Ø§Ù„ÙŠ ÙŠØ¹Ø²Ø² Ø§Ù„ÙˆØ¶ÙˆØ­",
        'sharp_details': "ØªÙØ§ØµÙŠÙ„ Ø­Ø§Ø¯Ø© ÙˆØ­ÙˆØ§Ù ÙˆØ§Ø¶Ø­Ø©", 
        'color_harmony': "ØªÙ†Ø§ØºÙ… Ù„ÙˆÙ†ÙŠ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø£Ø·ÙŠØ§Ù",
        'architectural_elements': "Ø¹Ù†Ø§ØµØ± Ù…Ø¹Ù…Ø§Ø±ÙŠØ© Ø¨ØªØµÙ…ÙŠÙ… Ù…Ù†Ø¸Ù…",
        'suitable_for': "Ù…Ù†Ø§Ø³Ø¨ Ù„Ù€",
        'professional_use': "Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ù‡Ù†ÙŠ",
        'high_quality': "Ø¬ÙˆØ¯Ø© Ø¹Ø§Ù„ÙŠØ©"
    },
    'en': {
        'technical_analysis': "Technical Analysis",
        'executive_summary': "Executive Summary", 
        'content_analysis': "Content Analysis",
        'recommendations': "Recommendations",
        'main_categories': "Main Categories",
        'visual_analysis': "Visual Analysis", 
        'resolution': "Resolution",
        'aspect_ratio': "Aspect Ratio",
        'dominant_colors': "Dominant Colors",
        'composition_type': "Composition Type",
        'brightness_level': "Brightness Level",
        'high_contrast': "High contrast enhances clarity",
        'sharp_details': "Sharp details and defined edges",
        'color_harmony': "Multi-spectral color harmony",
        'architectural_elements': "Structured architectural elements",
        'suitable_for': "Suitable for",
        'professional_use': "Professional use",
        'high_quality': "High quality"
    }
}

def translate_with_ia(text, target_lang='fr'):
    """Traduire du texte avec IA"""
    try:
        if not text or text.strip() == "":
            return text
        
        if target_lang == 'en':
            return text
        
        # DÃ©terminer la clÃ© du pipeline
        if target_lang == 'fr':
            pipeline_key = 'en-fr'
        elif target_lang == 'ar':
            pipeline_key = 'en-ar'
        else:
            return text
        
        # VÃ©rifier si le pipeline est chargÃ©
        if pipeline_key not in translation_pipelines:
            logger.warning(f"Pipeline {pipeline_key} non chargÃ©")
            return text
        
        # Traduire avec le pipeline
        pipeline_obj = translation_pipelines[pipeline_key]
        
        # Utiliser le bon format selon le modÃ¨le
        if pipeline_key == 'en-fr' and 't5' in str(pipeline_obj.model.__class__).lower():
            # Format pour T5
            input_text = f"translate English to French: {text}"
            result = pipeline_obj(input_text, max_length=100)[0]['translation_text']
        else:
            # Format standard pour les pipelines de traduction
            result = pipeline_obj(text, max_length=100)[0]['translation_text']
        
        logger.info(f"âœ… Traduction IA: '{text[:50]}...' -> '{result[:50]}...'")
        return result.strip()
        
    except Exception as e:
        logger.error(f"âŒ Erreur traduction IA: {e}")
        return text

def analyze_with_blip(image):
    """Analyser l'image avec BLIP"""
    try:
        if blip_model is None:
            logger.error("BLIP model not loaded")
            return None
            
        inputs = blip_processor(image, return_tensors="pt")
        with torch.no_grad():
            out = blip_model.generate(**inputs, max_length=100, num_beams=5)
        description = blip_processor.decode(out[0], skip_special_tokens=True)
        logger.info(f"BLIP description: {description}")
        return description
    except Exception as e:
        logger.error(f"Erreur BLIP: {e}")
        return None

def analyze_with_vit(image):
    """Analyser l'image avec ViT"""
    try:
        if vit_model is None:
            logger.error("ViT model not loaded")
            return None
            
        inputs = vit_processor(image, return_tensors="pt")
        with torch.no_grad():
            outputs = vit_model(**inputs)
        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
        top_prob, top_class = torch.topk(probabilities, 5)
        
        results = []
        for i in range(5):
            label = vit_model.config.id2label[top_class[0][i].item()]
            prob = top_prob[0][i].item()
            results.append({"label": label, "confidence": prob})
        
        logger.info(f"ViT results: {results}")
        return results
    except Exception as e:
        logger.error(f"Erreur ViT: {e}")
        return None

def analyze_colors_and_composition(image):
    """Analyser les couleurs et la composition"""
    try:
        width, height = image.size
        small_img = image.resize((100, 100))
        if small_img.mode != 'RGB':
            small_img = small_img.convert('RGB')
        
        colors = small_img.getcolors(10000)
        dominant_colors = []
        if colors:
            colors.sort(key=lambda x: x[0], reverse=True)
            for count, color in colors[:5]:
                hex_color = '#{:02x}{:02x}{:02x}'.format(color[0], color[1], color[2])
                dominant_colors.append(hex_color)
        
        stat = ImageStat.Stat(image)
        brightness = sum(stat.mean) / 3
        contrast = sum(stat.stddev) / 3
        
        edges = image.filter(ImageFilter.FIND_EDGES)
        edge_stat = ImageStat.Stat(edges)
        edge_strength = sum(edge_stat.mean) / 3
        
        return {
            'dominant_colors': dominant_colors,
            'brightness': brightness,
            'contrast': contrast,
            'edge_strength': edge_strength,
            'composition': analyze_composition_type(edge_strength, width/height),
            'width': width,
            'height': height
        }
    except Exception as e:
        logger.error(f"Erreur analyse couleurs: {e}")
        return {}

def analyze_composition_type(edge_strength, aspect_ratio):
    """Analyser le type de composition"""
    if edge_strength > 50:
        if aspect_ratio > 1.5:
            return "horizontal"
        elif aspect_ratio < 0.7:
            return "vertical" 
        else:
            return "balanced"
    else:
        if aspect_ratio > 1.3:
            return "panoramic"
        else:
            return "organic"

def generate_analysis_report(blip_description, vit_results, color_analysis, language='fr'):
    """GÃ©nÃ©rer le rapport d'analyse dans la langue choisie"""
    trans = TRANSLATIONS[language]
    
    if language == 'fr':
        return generate_french_report(blip_description, vit_results, color_analysis, trans)
    elif language == 'ar':
        return generate_arabic_report(blip_description, vit_results, color_analysis, trans)
    else:
        return generate_english_report(blip_description, vit_results, color_analysis, trans)

def translate_to_french(text):
    """Traduire le texte anglais en franÃ§ais avec IA"""
    if not text:
        return "Image contenant divers Ã©lÃ©ments visuels"
    
    # Essayer d'abord la traduction IA
    translated = translate_with_ia(text, 'fr')
    
    # Si la traduction IA a fonctionnÃ© et est diffÃ©rente du texte original
    if translated and translated != text:
        return translated
    
    # Fallback: traduction manuelle
    translations = {
        "a man with a stick in his hand in the jungle": "Un homme tenant un bÃ¢ton dans sa main dans la jungle",
        "in his hand": "dans sa main",
        "in the jungle": "dans la jungle",
        "with a stick": "tenant un bÃ¢ton",
        "a man": "un homme",
    }
    
    for eng, fr in translations.items():
        if eng in text.lower():
            return text.lower().replace(eng, fr).capitalize()
    
    return "Image contenant divers Ã©lÃ©ments visuels"

def translate_to_arabic(text):
    """Traduire le texte en arabe avec IA"""
    if not text:
        return "ØµÙˆØ±Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ù†Ø§ØµØ± Ø¨ØµØ±ÙŠØ© Ù…ØªÙ†ÙˆØ¹Ø©"
    
    # Essayer d'abord la traduction IA
    translated = translate_with_ia(text, 'ar')
    
    # Si la traduction IA a fonctionnÃ© et est diffÃ©rente du texte original
    if translated and translated != text:
        return translated
    
    # Fallback: traduction manuelle
    translations = {
        "a man with a stick in his hand in the jungle": "Ø±Ø¬Ù„ ÙŠØ­Ù…Ù„ Ø¹ØµØ§ ÙÙŠ ÙŠØ¯Ù‡ ÙÙŠ Ø§Ù„ØºØ§Ø¨Ø©",
        "in his hand": "ÙÙŠ ÙŠØ¯Ù‡",
        "in the jungle": "ÙÙŠ Ø§Ù„ØºØ§Ø¨Ø©",
        "with a stick": "ÙŠØ­Ù…Ù„ Ø¹ØµØ§",
        "a man": "Ø±Ø¬Ù„",
    }
    
    for eng, arb in translations.items():
        if eng in text.lower():
            return text.lower().replace(eng, arb)
    
    return "ØµÙˆØ±Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ù†Ø§ØµØ± Ø¨ØµØ±ÙŠØ© Ù…ØªÙ†ÙˆØ¹Ø©"

def generate_french_report(blip_description, vit_results, color_analysis, trans):
    """GÃ©nÃ©rer rapport en franÃ§ais"""
    report = "ğŸ–¼ï¸ RAPPORT COMPLET D'ANALYSE D'IMAGE\n\n"
    
    report += f"ğŸ“‹ {trans['executive_summary']}\n"
    french_desc = translate_to_french(blip_description) if blip_description else 'Image contenant divers Ã©lÃ©ments visuels'
    report += f"Description IA : {french_desc}\n\n"
    
    report += f"ğŸ” {trans['content_analysis']}\n"
    
    if vit_results:
        report += f"{trans['main_categories']} :\n"
        for i, result in enumerate(vit_results[:3], 1):
            label_fr = translate_label_to_french(result['label'])
            report += f"  â€¢ {label_fr} (confiance: {result['confidence']*100:.1f}%)\n"
    
    report += f"\n{trans['visual_analysis']} :\n"
    if color_analysis:
        report += f"  â€¢ {trans['resolution']} : {color_analysis['width']} Ã— {color_analysis['height']} pixels\n"
        report += f"  â€¢ {trans['aspect_ratio']} : {color_analysis['width']/color_analysis['height']:.2f}:1\n"
        report += f"  â€¢ {trans['dominant_colors']} : {', '.join(color_analysis['dominant_colors'][:3])}\n"
        report += f"  â€¢ {trans['composition_type']} : {get_composition_description(color_analysis['composition'], 'fr')}\n"
        report += f"  â€¢ {trans['brightness_level']} : {'Ã‰levÃ©' if color_analysis['brightness'] > 150 else 'Moyen' if color_analysis['brightness'] > 100 else 'Faible'}\n"
    
    report += f"\nğŸ¨ {trans['technical_analysis']}\n"
    report += generate_technical_analysis(color_analysis, vit_results, 'fr')
    
    report += f"\nğŸ’¡ {trans['recommendations']}\n"
    report += generate_recommendations(blip_description, vit_results, 'fr')
    
    return report

def generate_arabic_report(blip_description, vit_results, color_analysis, trans):
    """GÃ©nÃ©rer rapport en arabe"""
    report = "ğŸ–¼ï¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„ Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©\n\n"
    
    report += f"ğŸ“‹ {trans['executive_summary']}\n"
    arabic_desc = translate_to_arabic(blip_description) if blip_description else "ØµÙˆØ±Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ù†Ø§ØµØ± Ø¨ØµØ±ÙŠØ© Ù…ØªÙ†ÙˆØ¹Ø©"
    report += f"Ø§Ù„ÙˆØµÙ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ: {arabic_desc}\n\n"
    
    report += f"ğŸ” {trans['content_analysis']}\n"
    
    if vit_results:
        report += f"{trans['main_categories']}:\n"
        for i, result in enumerate(vit_results[:3], 1):
            label_ar = translate_label_to_arabic(result['label'])
            report += f"  â€¢ {label_ar} (Ø«Ù‚Ø©: {result['confidence']*100:.1f}%)\n"
    
    report += f"\n{trans['visual_analysis']}:\n"
    if color_analysis:
        report += f"  â€¢ {trans['resolution']} : {color_analysis['width']} Ã— {color_analysis['height']} Ø¨ÙƒØ³Ù„\n"
        report += f"  â€¢ {trans['aspect_ratio']} : {color_analysis['width']/color_analysis['height']:.2f}:1\n"
        report += f"  â€¢ {trans['dominant_colors']} : {', '.join(color_analysis['dominant_colors'][:3])}\n"
        report += f"  â€¢ {trans['composition_type']} : {get_composition_description(color_analysis['composition'], 'ar')}\n"
        report += f"  â€¢ {trans['brightness_level']} : {'Ù…Ø±ØªÙØ¹' if color_analysis['brightness'] > 150 else 'Ù…ØªÙˆØ³Ø·' if color_analysis['brightness'] > 100 else 'Ù…Ù†Ø®ÙØ¶'}\n"
    
    report += f"\nğŸ¨ {trans['technical_analysis']}\n"
    report += generate_technical_analysis(color_analysis, vit_results, 'ar')
    
    report += f"\nğŸ’¡ {trans['recommendations']}\n"
    report += generate_recommendations(blip_description, vit_results, 'ar')
    
    return report

def generate_english_report(blip_description, vit_results, color_analysis, trans):
    """GÃ©nÃ©rer rapport en anglais"""
    report = "ğŸ–¼ï¸ COMPREHENSIVE IMAGE ANALYSIS REPORT\n\n"
    
    report += f"ğŸ“‹ {trans['executive_summary']}\n"
    report += f"AI Description: {blip_description or 'Image containing various visual elements'}\n\n"
    
    report += f"ğŸ” {trans['content_analysis']}\n"
    
    if vit_results:
        report += f"{trans['main_categories']}:\n"
        for i, result in enumerate(vit_results[:3], 1):
            report += f"  â€¢ {result['label']} (confidence: {result['confidence']*100:.1f}%)\n"
    
    report += f"\n{trans['visual_analysis']}:\n"
    if color_analysis:
        report += f"  â€¢ {trans['resolution']}: {color_analysis['width']} Ã— {color_analysis['height']} pixels\n"
        report += f"  â€¢ {trans['aspect_ratio']}: {color_analysis['width']/color_analysis['height']:.2f}:1\n"
        report += f"  â€¢ {trans['dominant_colors']}: {', '.join(color_analysis['dominant_colors'][:3])}\n"
        report += f"  â€¢ {trans['composition_type']}: {get_composition_description(color_analysis['composition'], 'en')}\n"
        report += f"  â€¢ {trans['brightness_level']}: {'High' if color_analysis['brightness'] > 150 else 'Medium' if color_analysis['brightness'] > 100 else 'Low'}\n"
    
    report += f"\nğŸ¨ {trans['technical_analysis']}\n"
    report += generate_technical_analysis(color_analysis, vit_results, 'en')
    
    report += f"\nğŸ’¡ {trans['recommendations']}\n"
    report += generate_recommendations(blip_description, vit_results, 'en')
    
    return report

def translate_label_to_french(english_label):
    """Traduire les labels en franÃ§ais"""
    # D'abord essayer la traduction IA
    if 'en-fr' in translation_pipelines:
        translated = translate_with_ia(english_label, 'fr')
        if translated and translated != english_label:
            return translated
    
    # Fallback au dictionnaire
    translations = {
        "building": "BÃ¢timent",
        "street": "Rue", 
        "city": "Ville",
        "landscape": "Paysage",
        "mountain": "Montagne",
        "beach": "Plage",
        "forest": "ForÃªt",
        "person": "Personne",
        "car": "Voiture",
        "animal": "Animal",
        "tree": "Arbre",
        "water": "Eau",
        "sky": "Ciel",
        "food": "Nourriture",
        "indoor": "IntÃ©rieur",
        "outdoor": "ExtÃ©rieur",
        "zebra": "ZÃ¨bre",
        "gazelle": "Gazelle",
        "impala": "Impala",
        "giraffe": "Girafe",
        "comic book": "Bande dessinÃ©e",
        "book jacket": "Couverture de livre",
        "dust cover": "Couverture protectrice",
        "dust jacket": "Couverture de livre",
        "dust wrapper": "Emballage protecteur",
        "bow": "Arc",
        "book": "Livre"
    }
    
    return translations.get(english_label.lower(), english_label)

def translate_label_to_arabic(english_label):
    """Traduire les labels en arabe"""
    # D'abord essayer la traduction IA
    if 'en-ar' in translation_pipelines:
        translated = translate_with_ia(english_label, 'ar')
        if translated and translated != english_label:
            return translated
    
    # Fallback au dictionnaire
    translations = {
        "building": "Ù…Ø¨Ù†Ù‰",
        "street": "Ø´Ø§Ø±Ø¹",
        "city": "Ù…Ø¯ÙŠÙ†Ø©", 
        "landscape": "Ù…Ù†Ø¸Ø± Ø·Ø¨ÙŠØ¹ÙŠ",
        "mountain": "Ø¬Ø¨Ù„",
        "beach": "Ø´Ø§Ø·Ø¦",
        "forest": "ØºØ§Ø¨Ø©",
        "person": "Ø´Ø®Øµ",
        "car": "Ø³ÙŠØ§Ø±Ø©",
        "animal": "Ø­ÙŠÙˆØ§Ù†",
        "tree": "Ø´Ø¬Ø±Ø©",
        "water": "Ù…Ø§Ø¡",
        "sky": "Ø³Ù…Ø§Ø¡",
        "food": "Ø·Ø¹Ø§Ù…",
        "indoor": "Ø¯Ø§Ø®Ù„ÙŠ",
        "outdoor": "Ø®Ø§Ø±Ø¬ÙŠ",
        "zebra": "Ø­Ù…Ø§Ø± ÙˆØ­Ø´ÙŠ",
        "gazelle": "ØºØ²Ø§Ù„",
        "impala": "Ø¥Ù…Ø¨Ø§Ù„Ø§",
        "giraffe": "Ø²Ø±Ø§ÙØ©",
        "comic book": "ÙƒØªØ§Ø¨ Ù…ØµÙˆØ±",
        "book jacket": "ØºÙ„Ø§Ù ÙƒØªØ§Ø¨",
        "dust cover": "ØºØ·Ø§Ø¡ ÙˆØ§Ù‚ÙŠ",
        "dust jacket": "ØºÙ„Ø§Ù ÙƒØªØ§Ø¨",
        "dust wrapper": "ØºÙ„Ø§Ù ÙˆØ§Ù‚ÙŠ",
        "bow": "Ù‚ÙˆØ³",
        "book": "ÙƒØªØ§Ø¨"
    }
    
    return translations.get(english_label.lower(), english_label)

def get_composition_description(composition_type, language):
    """Obtenir la description de la composition"""
    descriptions = {
        'fr': {
            'horizontal': "Composition horizontale avec lignes fortes",
            'vertical': "Composition verticale avec Ã©lÃ©ments Ã©lancÃ©s", 
            'balanced': "Composition Ã©quilibrÃ©e et gÃ©omÃ©trique",
            'panoramic': "Paysage panoramique ouvert",
            'organic': "Composition fluide et organique"
        },
        'ar': {
            'horizontal': "ØªØ±ÙƒÙŠØ¨ Ø£ÙÙ‚ÙŠ Ù…Ø¹ Ø®Ø·ÙˆØ· Ù‚ÙˆÙŠØ©",
            'vertical': "ØªØ±ÙƒÙŠØ¨ Ø¹Ù…ÙˆØ¯ÙŠ Ù…Ø¹ Ø¹Ù†Ø§ØµØ± Ø±ÙÙŠØ¹Ø©",
            'balanced': "ØªØ±ÙƒÙŠØ¨ Ù…ØªÙˆØ§Ø²Ù† ÙˆÙ‡Ù†Ø¯Ø³ÙŠ", 
            'panoramic': "Ù…Ù†Ø¸Ø± Ø·Ø¨ÙŠØ¹ÙŠ Ø¨Ø§Ù†ÙˆØ±Ø§Ù…ÙŠ Ù…ÙØªÙˆØ­",
            'organic': "ØªØ±ÙƒÙŠØ¨ Ø³Ø§Ø¦Ù„ ÙˆØ¹Ø¶ÙˆÙŠ"
        },
        'en': {
            'horizontal': "Horizontal composition with strong lines",
            'vertical': "Vertical composition with slender elements",
            'balanced': "Balanced and geometric composition", 
            'panoramic': "Open panoramic landscape",
            'organic': "Fluid and organic composition"
        }
    }
    return descriptions[language].get(composition_type, composition_type)

def generate_technical_analysis(color_analysis, vit_results, language):
    """GÃ©nÃ©rer l'analyse technique"""
    trans = TRANSLATIONS[language]
    analysis = ""
    
    if color_analysis:
        if color_analysis['contrast'] > 60:
            analysis += f"  â€¢ {trans['high_contrast']}\n"
        if color_analysis['edge_strength'] > 40:
            analysis += f"  â€¢ {trans['sharp_details']}\n"
        if len(color_analysis['dominant_colors']) >= 3:
            analysis += f"  â€¢ {trans['color_harmony']}\n"
    
    if vit_results:
        main_category = vit_results[0]['label']
        if 'landscape' in main_category.lower():
            if language == 'fr':
                analysis += "  â€¢ Paysage panoramique avec profondeur de champ\n"
            elif language == 'ar':
                analysis += "  â€¢ Ù…Ù†Ø¸Ø± Ø·Ø¨ÙŠØ¹ÙŠ Ø¨Ø§Ù†ÙˆØ±Ø§Ù…ÙŠ Ø¨Ø¹Ù…Ù‚ Ù…Ø¬Ø§Ù„ÙŠ\n"
            else:
                analysis += "  â€¢ Panoramic landscape with field depth\n"
        elif 'building' in main_category.lower():
            analysis += f"  â€¢ {trans['architectural_elements']}\n"
        elif 'person' in main_category.lower():
            if language == 'fr':
                analysis += "  â€¢ PrÃ©sence d'Ã©lÃ©ments humains comme point focal\n"
            elif language == 'ar':
                analysis += "  â€¢ ÙˆØ¬ÙˆØ¯ Ø¹Ù†Ø§ØµØ± Ø¨Ø´Ø±ÙŠØ© ÙƒÙ…Ø±ÙƒØ² Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù…\n" 
            else:
                analysis += "  â€¢ Human elements as focal point\n"
    
    return analysis

def generate_recommendations(blip_desc, vit_results, language):
    """GÃ©nÃ©rer les recommandations"""
    trans = TRANSLATIONS[language]
    recommendations = []
    
    desc_lower = (blip_desc or "").lower()
    
    if any(word in desc_lower for word in ['landscape', 'mountain', 'beach', 'paysage', 'Ø¬Ø¨Ù„', 'Ø´Ø§Ø·Ø¦']):
        if language == 'fr':
            recommendations.extend(["PrÃ©sentations environnementales", "ArriÃ¨re-plans naturels"])
        elif language == 'ar':
            recommendations.extend(["Ø¹Ø±ÙˆØ¶ Ø¨ÙŠØ¦ÙŠØ©", "Ø®Ù„ÙÙŠØ§Øª Ø·Ø¨ÙŠØ¹ÙŠØ©"])
        else:
            recommendations.extend(["Environmental presentations", "Natural backgrounds"])
    
    if any(word in desc_lower for word in ['city', 'building', 'street', 'ville', 'bÃ¢timent', 'Ù…Ø¯ÙŠÙ†Ø©', 'Ù…Ø¨Ù†Ù‰']):
        if language == 'fr':
            recommendations.extend(["Projets urbains", "Design architectural"])
        elif language == 'ar':
            recommendations.extend(["Ù…Ø´Ø§Ø±ÙŠØ¹ Ø¹Ù…Ø±Ø§Ù†ÙŠØ©", "ØªØµÙ…ÙŠÙ… Ù…Ø¹Ù…Ø§Ø±ÙŠ"])
        else:
            recommendations.extend(["Urban projects", "Architectural design"])
    
    if language == 'fr':
        recommendations.extend(["Usage professionnel", "Impression et affichage numÃ©rique"])
    elif language == 'ar':
        recommendations.extend(["Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù‡Ù†ÙŠ", "Ø·Ø¨Ø§Ø¹Ø© ÙˆØ¹Ø±Ø¶ Ø±Ù‚Ù…ÙŠ"])
    else:
        recommendations.extend(["Professional use", "Print and digital display"])
    
    return "  â€¢ " + "\n  â€¢ ".join(recommendations[:4])

@app.route('/api/analyze-image', methods=['POST'])
def analyze_image():
    """Endpoint pour analyser l'image"""
    try:
        # Attendre que les modÃ¨les soient chargÃ©s
        if blip_model is None or vit_model is None:
            return jsonify({
                'success': False, 
                'error': 'Models are still loading. Please wait a moment and try again.'
            }), 503
        
        if 'image' not in request.files:
            return jsonify({'success': False, 'error': 'No image provided'}), 400
        
        file = request.files['image']
        language = request.form.get('language', 'fr')
        
        logger.info(f"ğŸŒ Analyse demandÃ©e en: {language}")
        
        if file.filename == '':
            return jsonify({'success': False, 'error': 'No file selected'}), 400
        
        allowed_formats = ['jpg', 'jpeg', 'png', 'webp', 'bmp', 'gif']
        if not any(file.filename.lower().endswith(f'.{fmt}') for fmt in allowed_formats):
            return jsonify({
                'success': False, 
                'error': f'Format not supported. Use: {", ".join(allowed_formats)}'
            }), 400
        
        try:
            image = Image.open(file.stream)
            if image.mode != 'RGB':
                image = image.convert('RGB')
        except Exception as e:
            return jsonify({'success': False, 'error': 'Invalid or corrupted image file'}), 400
        
        # Analyse avec l'IA
        blip_description = analyze_with_blip(image) if blip_model else None
        vit_results = analyze_with_vit(image) if vit_model else None
        color_analysis = analyze_colors_and_composition(image)
        
        # GÃ©nÃ©rer le rapport
        analysis_report = generate_analysis_report(
            blip_description, 
            vit_results, 
            color_analysis, 
            language
        )
        
        response_data = {
            'success': True,
            'description': analysis_report,
            'details': {
                'confidence': 0.95,
                'dominant_colors': color_analysis.get('dominant_colors', []),
                'image_format': image.format,
                'dimensions': f"{color_analysis.get('width', 0)}x{color_analysis.get('height', 0)}",
                'analysis_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'language': language,
                'models_used': ['BLIP', 'ViT'],
                'translation_ia': len(translation_pipelines) > 0
            }
        }
        
        logger.info(f"âœ… Image analysÃ©e: {file.filename} | Langue: {language}")
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"âŒ Erreur analyse: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/health', methods=['GET'])
def health_check():
    return jsonify({
        'status': 'healthy âœ…' if blip_model and vit_model else 'loading â³',
        'service': 'Multi-Language Image Analysis API',
        'version': '6.0.0',
        'supported_languages': ['fr', 'ar', 'en'],
        'models_loaded': {
            'blip': blip_model is not None,
            'vit': vit_model is not None,
            'translation_ia': len(translation_pipelines) > 0
        },
        'translation_pipelines': list(translation_pipelines.keys()),
        'timestamp': datetime.now().isoformat()
    })

@app.route('/api/test-translation', methods=['POST'])
def test_translation():
    """Endpoint pour tester la traduction IA"""
    try:
        data = request.json
        text = data.get('text', 'a man with a stick in his hand in the jungle')
        language = data.get('language', 'ar')
        
        logger.info(f"Test traduction: '{text}' -> {language}")
        
        if language == 'fr':
            translated = translate_to_french(text)
        elif language == 'ar':
            translated = translate_to_arabic(text)
        else:
            translated = text
        
        return jsonify({
            'success': True,
            'original': text,
            'translated': translated,
            'language': language,
            'translation_method': 'IA' if len(translation_pipelines) > 0 else 'Dictionary'
        })
    except Exception as e:
        logger.error(f"Erreur test traduction: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

if __name__ == '__main__':
    print("ğŸš€ DÃ©marrage du serveur d'analyse multi-langues...")
    print("ğŸ”— URL: http://localhost:8000")
    print("ğŸŒ Langues supportÃ©es: FranÃ§ais, Arabe, Anglais")
    print("ğŸ¤– Chargement des modÃ¨les en arriÃ¨re-plan...")
    print("ğŸ“¸ Le serveur dÃ©marre, les modÃ¨les se chargeront progressivement")
    print("\nğŸ“‹ Endpoints disponibles:")
    print("   â€¢ POST /api/analyze-image - Analyser une image")
    print("   â€¢ GET  /api/health - VÃ©rifier l'Ã©tat des modÃ¨les")
    print("   â€¢ POST /api/test-translation - Tester la traduction IA")
    
    # Attendre un peu que le chargement commence
    time.sleep(2)
    
    app.run(host='0.0.0.0', port=8000, debug=False)